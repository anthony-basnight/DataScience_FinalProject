{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce42611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red train D:   (1200, 11)\n",
      "Red train y:   (1200,)\n",
      "Red test D:    (399, 11)\n",
      "Red test y:    (399,)\n",
      "White train D: (3675, 11)\n",
      "White train y: (3675,)\n",
      "White test D:  (1223, 11)\n",
      "White test y:  (1223,)\n",
      "Column headers: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "\n",
    "\n",
    "# Load the data sets\n",
    "D_red   = np.loadtxt(open(\"winequality-red.csv\"), delimiter = \";\", skiprows = 1)\n",
    "D_white = np.loadtxt(open(\"winequality-white.csv\"), delimiter = \";\", skiprows = 1)\n",
    "\n",
    "\n",
    "red_cols = open(\"winequality-red.csv\", \"r\").readline().replace(\"\\n\", \"\").replace('\"', '').split(\";\")\n",
    "white_cols = open(\"winequality-white.csv\", \"r\").readline().replace(\"\\n\", \"\").replace('\"', '').split(\";\")\n",
    "\n",
    "\n",
    "np.random.shuffle(D_red)\n",
    "np.random.shuffle(D_white)\n",
    "\n",
    "\n",
    "# 75% train, 25% test\n",
    "D_red_train   = D_red[:1200]\n",
    "D_red_test    = D_red[1200:]\n",
    "\n",
    "D_white_train = D_white[:3675]\n",
    "D_white_test  = D_white[3675:]\n",
    "\n",
    "\n",
    "# Separate features and actual quality\n",
    "y_red_train = D_red_train[:, 11]\n",
    "D_red_train = np.delete(D_red_train, 11, 1)\n",
    "y_red_test  = D_red_test[:, 11]\n",
    "D_red_test  = np.delete(D_red_test, 11, 1)\n",
    "\n",
    "y_white_train = D_white_train[:, 11]\n",
    "D_white_train = np.delete(D_white_train, 11, 1)\n",
    "y_white_test  = D_white_test[:, 11]\n",
    "D_white_test  = np.delete(D_white_test, 11, 1)\n",
    "\n",
    "# Check shapes of data frames\n",
    "print(\"Red train D:  \", D_red_train.shape)\n",
    "print(\"Red train y:  \", y_red_train.shape)\n",
    "print(\"Red test D:   \", D_red_test.shape)\n",
    "print(\"Red test y:   \", y_red_test.shape)\n",
    "\n",
    "print(\"White train D:\", D_white_train.shape)\n",
    "print(\"White train y:\", y_white_train.shape)\n",
    "print(\"White test D: \", D_white_test.shape)\n",
    "print(\"White test y: \", y_white_test.shape)\n",
    "\n",
    "print(\"Column headers:\", red_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the features for both datasets (helps improve model performance)\n",
    "scaler = StandardScaler()\n",
    "X_red_train = scaler.fit_transform(D_red_train)\n",
    "X_red_test = scaler.transform(D_red_test)\n",
    "X_white_train = scaler.fit_transform(D_white_train)\n",
    "X_white_test = scaler.transform(D_white_test)\n",
    "\n",
    "# Convert quality ratings to binary classification (0 = 7 or below, 1 = above 7)\n",
    "# For red wine\n",
    "y_red_train_binary = (y_red_train > 6).astype(int)\n",
    "y_red_test_binary = (y_red_test > 6).astype(int)\n",
    "\n",
    "# For white wine\n",
    "y_white_train_binary = (y_white_train > 6).astype(int)\n",
    "y_white_test_binary = (y_white_test > 6).astype(int)\n",
    "\n",
    "# Create new logistic regression models for binary classification\n",
    "model_red_binary = LogisticRegression(max_iter=500)\n",
    "model_white_binary = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the binary models\n",
    "model_red_binary.fit(X_red_train, y_red_train_binary)\n",
    "model_white_binary.fit(X_white_train, y_white_train_binary)\n",
    "\n",
    "# Make predictions on the test set for binary classification\n",
    "y_red_pred_binary = model_red_binary.predict(X_red_test)\n",
    "y_white_pred_binary = model_white_binary.predict(X_white_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d75f707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED WINE BINARY CLASSIFICATION\n",
      "    Accuracy: 0.8696741854636592\n",
      "    Accuracy: 0.7744360902255639\n",
      "    Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.93       342\n",
      "           1       0.59      0.30      0.40        57\n",
      "           0       0.98      0.77      0.86       365\n",
      "           1       0.25      0.79      0.38        34\n",
      "\n",
      "    accuracy                           0.87       399\n",
      "   macro avg       0.74      0.63      0.66       399\n",
      "weighted avg       0.85      0.87      0.85       399\n",
      "    accuracy                           0.77       399\n",
      "   macro avg       0.61      0.78      0.62       399\n",
      "weighted avg       0.91      0.77      0.82       399\n",
      "\n",
      "    Confusion matrix:\n",
      "[[330  12]\n",
      " [ 40  17]]\n",
      "[[282  83]\n",
      " [  7  27]]\n",
      "    Attribute weights:\n",
      "        0.709 \t- alcohol\n",
      "        0.654 \t- sulphates\n",
      "        0.553 \t- fixed acidity\n",
      "        0.36 \t- residual sugar\n",
      "        0.117 \t- citric acid\n",
      "        0.108 \t- pH\n",
      "        0.033 \t- free sulfur dioxide\n",
      "        -0.43 \t- total sulfur dioxide\n",
      "        -0.464 \t- volatile acidity\n",
      "        -0.554 \t- density\n",
      "        -0.609 \t- chlorides\n",
      "        0.987 \t- fixed acidity\n",
      "        0.955 \t- alcohol\n",
      "        0.623 \t- sulphates\n",
      "        0.513 \t- residual sugar\n",
      "        0.215 \t- pH\n",
      "        0.086 \t- free sulfur dioxide\n",
      "        -0.135 \t- citric acid\n",
      "        -0.214 \t- chlorides\n",
      "        -0.399 \t- volatile acidity\n",
      "        -0.484 \t- total sulfur dioxide\n",
      "        -0.794 \t- density\n",
      "\n",
      "WHITE WINE BINARY CLASSIFICATION\n",
      "    Accuracy: 0.8192968111201963\n",
      "    Accuracy: 0.5944399018806215\n",
      "    Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.89       970\n",
      "           1       0.65      0.27      0.38       253\n",
      "           0       0.93      0.55      0.69      1004\n",
      "           1       0.28      0.80      0.42       219\n",
      "\n",
      "    accuracy                           0.82      1223\n",
      "   macro avg       0.74      0.62      0.64      1223\n",
      "weighted avg       0.80      0.82      0.79      1223\n",
      "    accuracy                           0.59      1223\n",
      "   macro avg       0.60      0.68      0.55      1223\n",
      "weighted avg       0.81      0.59      0.64      1223\n",
      "\n",
      "    Confusion matrix:\n",
      "[[933  37]\n",
      " [184  69]]\n",
      "[[551 453]\n",
      " [ 43 176]]\n",
      "    Attribute weights:\n",
      "        1.46 \t- residual sugar\n",
      "        0.496 \t- pH\n",
      "        0.458 \t- fixed acidity\n",
      "        0.225 \t- sulphates\n",
      "        0.164 \t- alcohol\n",
      "        0.148 \t- free sulfur dioxide\n",
      "        -0.013 \t- total sulfur dioxide\n",
      "        -0.088 \t- citric acid\n",
      "        -0.205 \t- chlorides\n",
      "        -0.366 \t- volatile acidity\n",
      "        -1.949 \t- density\n",
      "        1.415 \t- residual sugar\n",
      "        0.499 \t- alcohol\n",
      "        0.46 \t- pH\n",
      "        0.341 \t- fixed acidity\n",
      "        0.285 \t- sulphates\n",
      "        0.162 \t- free sulfur dioxide\n",
      "        -0.041 \t- total sulfur dioxide\n",
      "        -0.119 \t- citric acid\n",
      "        -0.137 \t- chlorides\n",
      "        -0.445 \t- volatile acidity\n",
      "        -1.71 \t- density\n",
      "\n",
      "AVG ATTRIBUTE WEIGHTS\n",
      "        0.91 \t- residual sugar\n",
      "        0.506 \t- fixed acidity\n",
      "        0.44 \t- sulphates\n",
      "        0.436 \t- alcohol\n",
      "        0.302 \t- pH\n",
      "        0.09 \t- free sulfur dioxide\n",
      "        0.015 \t- citric acid\n",
      "        -0.222 \t- total sulfur dioxide\n",
      "        -0.407 \t- chlorides\n",
      "        -0.415 \t- volatile acidity\n",
      "        -1.252 \t- density\n",
      "\n",
      "ATTRIBUTE IMPACT RATING (average position from sorted attribute weights, lower = positive impact)\n",
      "        2.5 \t- residual sugar\n",
      "        3.0 \t- alcohol\n",
      "        3.0 \t- sulphates\n",
      "        3.0 \t- fixed acidity\n",
      "        4.0 \t- pH\n",
      "        6.5 \t- citric acid\n",
      "        6.5 \t- free sulfur dioxide\n",
      "        7.5 \t- total sulfur dioxide\n",
      "        9.5 \t- volatile acidity\n",
      "        10.0 \t- chlorides\n",
      "        10.5 \t- density\n"
      "        0.964 \t- residual sugar\n",
      "        0.727 \t- alcohol\n",
      "        0.664 \t- fixed acidity\n",
      "        0.454 \t- sulphates\n",
      "        0.338 \t- pH\n",
      "        0.124 \t- free sulfur dioxide\n",
      "        -0.127 \t- citric acid\n",
      "        -0.176 \t- chlorides\n",
      "        -0.262 \t- total sulfur dioxide\n",
      "        -0.422 \t- volatile acidity\n",
      "        -1.252 \t- density\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display performance metrics for binary classification\n",
    "red_accuracy_binary = accuracy_score(y_red_test_binary, y_red_pred_binary)\n",
    "white_accuracy_binary = accuracy_score(y_white_test_binary, y_white_pred_binary)\n",
    "red_report_binary = classification_report(y_red_test_binary, y_red_pred_binary)\n",
    "white_report_binary = classification_report(y_white_test_binary, y_white_pred_binary)\n",
    "\n",
    "red_attr_weights = {}\n",
    "white_attr_weights = {}\n",
    "avg_attr_weights = {}\n",
    "attr_impact_rating = {}\n",
    "for i in range(len(red_cols) - 1):\n",
    "    red_attr_weights[red_cols[i]] = round(model_red_binary.coef_[0][i], 3)\n",
    "    white_attr_weights[white_cols[i]] = round(model_white_binary.coef_[0][i], 3)    \n",
    "    avg_attr_weights[red_cols[i]] = (red_attr_weights[red_cols[i]] + white_attr_weights[red_cols[i]]) / 2\n",
    "\n",
    "print(\"RED WINE BINARY CLASSIFICATION\")\n",
    "print(\"    Accuracy:\", red_accuracy_binary)\n",
    "print(\"    Report:\\n\" + str(red_report_binary))\n",
    "print(\"    Confusion matrix:\\n\" + str(confusion_matrix(y_red_test_binary, y_red_pred_binary)))\n",
    "print(\"    Attribute weights:\")\n",
    "c = 1\n",
    "for i in sorted(red_attr_weights.items(), key=lambda x: -(x[1])):\n",
    "    print(\"        \" + str(i[1]), \"\\t-\", i[0])\n",
    "    attr_impact_rating[i[0]] = c\n",
    "    c += 1\n",
    "\n",
    "print()\n",
    "print(\"WHITE WINE BINARY CLASSIFICATION\")\n",
    "print(\"    Accuracy:\", white_accuracy_binary)\n",
    "print(\"    Report:\\n\" + str(white_report_binary))\n",
    "print(\"    Confusion matrix:\\n\" + str(confusion_matrix(y_white_test_binary, y_white_pred_binary)))\n",
    "print(\"    Attribute weights:\")\n",
    "c = 1\n",
    "for i in sorted(white_attr_weights.items(), key=lambda x: -(x[1])):\n",
    "    print(\"        \" + str(i[1]), \"\\t-\", i[0])\n",
    "    attr_impact_rating[i[0]] += c\n",
    "    attr_impact_rating[i[0]] /= 2\n",
    "    c += 1\n",
    "\n",
    "print()\n",
    "print(\"AVG ATTRIBUTE WEIGHTS\")\n",
    "for i in sorted(avg_attr_weights.items(), key=lambda x: -x[1]):\n",
    "    print(\"        \" + str(round(i[1], 3)), \"\\t-\", i[0])\n",
    "print()\n",
    "print(\"ATTRIBUTE IMPACT RATING (average position from sorted attribute weights, lower = positive impact)\")\n",
    "for i in sorted(attr_impact_rating.items(), key=lambda x: x[1]):\n",
    "    print(\"        \" + str(i[1]), \"\\t-\", i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3926a5",
   "metadata": {},
   "source": [
    "# Initial Questions\n",
    "### 1. Which attributes affect the quality rating more than others?\n",
    "\n",
    "\n",
    "\n",
    "### 2. Given its attributes, is it possible to predict if a wine will be \"high\" quality (>6)?\n",
    "\n",
    "Originally, we wanted to see if we could predict the exact rating of a wine, but this quickly proved to be tough. Our model wasn't yielding high accuracy (~40%). This could be due to a number of reasons, but the most probable is that the data may be too noisy for a relatively simple model to predict the quality with high accuracy. \n",
    "\n",
    "We then tried putting the ratings in \"buckets\" and have the model try to predict the score within a range. The buckets were quality scores of 5 and below, 6 or 7, and 8 and above. This is due to the fact that the largest concentration of quality scores in the dataset are in the 6-8 range. This approach yielded slightly higher accuracy than the first idea, getting closer to 60%.\n",
    "\n",
    "Next, we decided to try to predict if a quality score will be higher than a 7. This would simplify the work the model has to do by giving it a binary classification problem. Using this approach, our model was able to reach 98.7% accuracy with the red wine dataset (1,200 training points, 399 testing points), and 97.9% accuracy with the white wine dataset (3,675 training points, 1,223 testing points). The only problem, was that the model wasn't actually doing any guessing. Due to the nature of the dataset, it just so happens that 98% of the data points have a quality rating of 7 or less, so the model was simply picking 7 or less for everything and managing to get 98% accurate. The white wine model picked 8 or higher twice, and the red wine model never picked 8 or higher at all.\n",
    "\n",
    "Finally, to have a model that actually made decisions, we lowered the bar for binary classification by having the model predict if the quality score would be higher than a 6. This lowered the accuracy from the previous model (**90.0%** for red wine, **78.5%** for white wine), but at least the model was actually trying to predict when the quality rating was 7 or greater."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965cb03c",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Final accuracy on testing data: **90.0%** for red wine, **78.5%** for white wine\n",
    "\n",
    "Given the features of red wine, we are able to accurately predict if the quality rating of the wine is going greater than a 6. \n",
    "\n",
    "Given the features of white wine, we are able to *relatively* accurately predict if the quality rating of the wine is going greater than a 6. \n",
    "\n",
    "This can be used to identify common characteristics of quality wine, and to give a sense of if a new wine will be good before tasting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43b2145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\shane\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\shane\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from imbalanced-learn->imblearn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\shane\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\shane\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shane\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.12.4 imblearn-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Shane\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a7895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8370927318295739,\n",
       " 0.7252657399836467,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.95      0.87      0.91       365\\n           1       0.25      0.47      0.33        34\\n\\n    accuracy                           0.84       399\\n   macro avg       0.60      0.67      0.62       399\\nweighted avg       0.89      0.84      0.86       399\\n',\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.90      0.75      0.82      1004\\n           1       0.35      0.61      0.44       219\\n\\n    accuracy                           0.73      1223\\n   macro avg       0.62      0.68      0.63      1223\\nweighted avg       0.80      0.73      0.75      1223\\n')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Step 1: Apply SMOTE to balance the training datasets\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# For red wine\n",
    "X_red_train_balanced, y_red_train_balanced = smote.fit_resample(X_red_train, y_red_train_binary)\n",
    "\n",
    "# For white wine\n",
    "X_white_train_balanced, y_white_train_balanced = smote.fit_resample(X_white_train, y_white_train_binary)\n",
    "\n",
    "# Step 2: Train ensemble classifiers\n",
    "# Random Forest for red wine\n",
    "rf_model_red = RandomForestClassifier(random_state=42)\n",
    "rf_model_red.fit(X_red_train_balanced, y_red_train_balanced)\n",
    "\n",
    "# Random Forest for white wine\n",
    "rf_model_white = RandomForestClassifier(random_state=42)\n",
    "rf_model_white.fit(X_white_train_balanced, y_white_train_balanced)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_red_pred_rf = rf_model_red.predict(X_red_test)\n",
    "y_white_pred_rf = rf_model_white.predict(X_white_test)\n",
    "\n",
    "# Calculate accuracy and classification reports for the Random Forest models\n",
    "red_rf_accuracy = accuracy_score(y_red_test_binary, y_red_pred_rf)\n",
    "white_rf_accuracy = accuracy_score(y_white_test_binary, y_white_pred_rf)\n",
    "red_rf_report = classification_report(y_red_test_binary, y_red_pred_rf)\n",
    "white_rf_report = classification_report(y_white_test_binary, y_white_pred_rf)\n",
    "\n",
    "print(\"RED WINE RANDOM FOREST\", red_rf_accuracy)\n",
    "print(red_rf_report)\n",
    "print(\"WHITE WINE RANDOM FOREST\", white_rf_accuracy)\n",
    "print(white_rf_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
