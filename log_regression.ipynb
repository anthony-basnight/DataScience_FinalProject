{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce42611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red train D:   (1200, 11)\n",
      "Red train y:   (1200,)\n",
      "Red test D:    (399, 11)\n",
      "Red test y:    (399,)\n",
      "White train D: (3675, 11)\n",
      "White train y: (3675,)\n",
      "White test D:  (1223, 11)\n",
      "White test y:  (1223,)\n",
      "Column headers: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "\n",
    "\n",
    "# Load the data sets\n",
    "D_red   = np.loadtxt(open(\"winequality-red.csv\"), delimiter = \";\", skiprows = 1)\n",
    "D_white = np.loadtxt(open(\"winequality-white.csv\"), delimiter = \";\", skiprows = 1)\n",
    "\n",
    "\n",
    "red_cols = open(\"winequality-red.csv\", \"r\").readline().replace(\"\\n\", \"\").replace('\"', '').split(\";\")\n",
    "white_cols = open(\"winequality-white.csv\", \"r\").readline().replace(\"\\n\", \"\").replace('\"', '').split(\";\")\n",
    "\n",
    "\n",
    "# Shuffle the datasets\n",
    "np.random.shuffle(D_red)\n",
    "np.random.shuffle(D_white)\n",
    "\n",
    "\n",
    "# 75% train, 25% test\n",
    "D_red_train   = D_red[:1200]\n",
    "D_red_test    = D_red[1200:]\n",
    "\n",
    "D_white_train = D_white[:3675]\n",
    "D_white_test  = D_white[3675:]\n",
    "\n",
    "\n",
    "# Separate features and actual quality\n",
    "y_red_train = D_red_train[:, 11]\n",
    "D_red_train = np.delete(D_red_train, 11, 1)\n",
    "y_red_test  = D_red_test[:, 11]\n",
    "D_red_test  = np.delete(D_red_test, 11, 1)\n",
    "\n",
    "y_white_train = D_white_train[:, 11]\n",
    "D_white_train = np.delete(D_white_train, 11, 1)\n",
    "y_white_test  = D_white_test[:, 11]\n",
    "D_white_test  = np.delete(D_white_test, 11, 1)\n",
    "\n",
    "# Check shapes of data frames\n",
    "print(\"Red train D:  \", D_red_train.shape)\n",
    "print(\"Red train y:  \", y_red_train.shape)\n",
    "print(\"Red test D:   \", D_red_test.shape)\n",
    "print(\"Red test y:   \", y_red_test.shape)\n",
    "\n",
    "print(\"White train D:\", D_white_train.shape)\n",
    "print(\"White train y:\", y_white_train.shape)\n",
    "print(\"White test D: \", D_white_test.shape)\n",
    "print(\"White test y: \", y_white_test.shape)\n",
    "\n",
    "print(\"Column headers:\", red_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "918d3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the features for both datasets (helps improve model performance)\n",
    "scaler = StandardScaler()\n",
    "X_red_train = scaler.fit_transform(D_red_train)\n",
    "X_red_test = scaler.transform(D_red_test)\n",
    "X_white_train = scaler.fit_transform(D_white_train)\n",
    "X_white_test = scaler.transform(D_white_test)\n",
    "\n",
    "# Convert quality ratings to binary classification (0 = 7 or below, 1 = above 7)\n",
    "# For red wine\n",
    "y_red_train_binary = (y_red_train > 6).astype(int)\n",
    "y_red_test_binary = (y_red_test > 6).astype(int)\n",
    "\n",
    "# For white wine\n",
    "y_white_train_binary = (y_white_train > 6).astype(int)\n",
    "y_white_test_binary = (y_white_test > 6).astype(int)\n",
    "\n",
    "# Create new logistic regression models for binary classification\n",
    "model_red_binary = LogisticRegression(max_iter=500)\n",
    "model_white_binary = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the binary models\n",
    "model_red_binary.fit(X_red_train, y_red_train_binary)\n",
    "model_white_binary.fit(X_white_train, y_white_train_binary)\n",
    "\n",
    "# Make predictions on the test set for binary classification\n",
    "y_red_pred_binary = model_red_binary.predict(X_red_test)\n",
    "y_white_pred_binary = model_white_binary.predict(X_white_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d75f707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED WINE BINARY CLASSIFICATION\n",
      "    Accuracy: 0.8922305764411027\n",
      "    Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94       352\n",
      "           1       0.61      0.23      0.34        47\n",
      "\n",
      "    accuracy                           0.89       399\n",
      "   macro avg       0.76      0.61      0.64       399\n",
      "weighted avg       0.87      0.89      0.87       399\n",
      "\n",
      "    Confusion matrix:\n",
      "[[345   7]\n",
      " [ 36  11]]\n",
      "    Attribute weights:\n",
      "        0.828 \t- alcohol\n",
      "        0.675 \t- sulphates\n",
      "        0.402 \t- residual sugar\n",
      "        0.391 \t- fixed acidity\n",
      "        0.12 \t- free sulfur dioxide\n",
      "        0.064 \t- citric acid\n",
      "        0.006 \t- pH\n",
      "        -0.345 \t- chlorides\n",
      "        -0.378 \t- volatile acidity\n",
      "        -0.48 \t- density\n",
      "        -0.689 \t- total sulfur dioxide\n",
      "\n",
      "WHITE WINE BINARY CLASSIFICATION\n",
      "    Accuracy: 0.8021259198691741\n",
      "    Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88       952\n",
      "           1       0.62      0.28      0.39       271\n",
      "\n",
      "    accuracy                           0.80      1223\n",
      "   macro avg       0.72      0.62      0.63      1223\n",
      "weighted avg       0.78      0.80      0.77      1223\n",
      "\n",
      "    Confusion matrix:\n",
      "[[905  47]\n",
      " [195  76]]\n",
      "    Attribute weights:\n",
      "        1.236 \t- residual sugar\n",
      "        0.45 \t- pH\n",
      "        0.407 \t- fixed acidity\n",
      "        0.386 \t- alcohol\n",
      "        0.251 \t- sulphates\n",
      "        0.211 \t- free sulfur dioxide\n",
      "        -0.054 \t- citric acid\n",
      "        -0.083 \t- total sulfur dioxide\n",
      "        -0.33 \t- chlorides\n",
      "        -0.355 \t- volatile acidity\n",
      "        -1.543 \t- density\n",
      "\n",
      "AVG ATTRIBUTE WEIGHTS\n",
      "        0.819 \t- residual sugar\n",
      "        0.607 \t- alcohol\n",
      "        0.463 \t- sulphates\n",
      "        0.399 \t- fixed acidity\n",
      "        0.228 \t- pH\n",
      "        0.165 \t- free sulfur dioxide\n",
      "        0.005 \t- citric acid\n",
      "        -0.338 \t- chlorides\n",
      "        -0.366 \t- volatile acidity\n",
      "        -0.386 \t- total sulfur dioxide\n",
      "        -1.011 \t- density\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display performance metrics for binary classification\n",
    "red_accuracy_binary = accuracy_score(y_red_test_binary, y_red_pred_binary)\n",
    "white_accuracy_binary = accuracy_score(y_white_test_binary, y_white_pred_binary)\n",
    "red_report_binary = classification_report(y_red_test_binary, y_red_pred_binary)\n",
    "white_report_binary = classification_report(y_white_test_binary, y_white_pred_binary)\n",
    "\n",
    "red_attr_weights = {}\n",
    "white_attr_weights = {}\n",
    "avg_attr_weights = {}\n",
    "for i in range(len(red_cols) - 1):\n",
    "    red_attr_weights[red_cols[i]] = round(model_red_binary.coef_[0][i], 3)\n",
    "    white_attr_weights[white_cols[i]] = round(model_white_binary.coef_[0][i], 3)    \n",
    "    avg_attr_weights[red_cols[i]] = (red_attr_weights[red_cols[i]] + white_attr_weights[red_cols[i]]) / 2\n",
    "\n",
    "print(\"RED WINE BINARY CLASSIFICATION\")\n",
    "print(\"    Accuracy:\", red_accuracy_binary)\n",
    "print(\"    Report:\\n\" + str(red_report_binary))\n",
    "print(\"    Confusion matrix:\\n\" + str(confusion_matrix(y_red_test_binary, y_red_pred_binary)))\n",
    "print(\"    Attribute weights:\")\n",
    "for i in sorted(red_attr_weights.items(), key=lambda x: -(x[1])):\n",
    "    print(\"        \" + str(i[1]), \"\\t-\", i[0])\n",
    "\n",
    "print()\n",
    "print(\"WHITE WINE BINARY CLASSIFICATION\")\n",
    "print(\"    Accuracy:\", white_accuracy_binary)\n",
    "print(\"    Report:\\n\" + str(white_report_binary))\n",
    "print(\"    Confusion matrix:\\n\" + str(confusion_matrix(y_white_test_binary, y_white_pred_binary)))\n",
    "print(\"    Attribute weights:\")\n",
    "for i in sorted(white_attr_weights.items(), key=lambda x: -(x[1])):\n",
    "    print(\"        \" + str(i[1]), \"\\t-\", i[0])\n",
    "\n",
    "print()\n",
    "print(\"AVG ATTRIBUTE WEIGHTS\")\n",
    "for i in sorted(avg_attr_weights.items(), key=lambda x: -x[1]):\n",
    "    print(\"        \" + str(round(i[1], 3)), \"\\t-\", i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3926a5",
   "metadata": {},
   "source": [
    "# Initial Questions\n",
    "### 1. Which attributes affect the quality rating more than others?\n",
    "\n",
    "\n",
    "\n",
    "### 2. Given its attributes, is it possible to predict if a wine will be \"high\" quality (>7)?\n",
    "\n",
    "Originally, we wanted to see if we could predict the exact rating of a wine, but this quickly proved to be tough. Our model wasn't yielding high accuracy (~40%). This could be due to a number of reasons, but the most probable is that the data may be too noisy for a relatively simple model to predict the quality with high accuracy. \n",
    "\n",
    "We then tried putting the ratings in \"buckets\" and have the model try to predict the score within a range. The buckets were quality scores of 5 and below, 6 or 7, and 8 and above. This is due to the fact that the largest concentration of quality scores in the dataset are in the 6-8 range. This approach yielded slightly higher accuracy than the first idea, getting closer to 60%.\n",
    "\n",
    "Next, we decided to try to predict if a quality score will be higher than a 7. This would simplify the work the model has to do by giving it a binary classification problem. Using this approach, our model was able to reach 98.7% accuracy with the red wine dataset (1,200 training points, 399 testing points), and 97.9% accuracy with the white wine dataset (3,675 training points, 1,223 testing points). The only problem, was that the model wasn't actually doing any guessing. Due to the nature of the dataset, it just so happens that 98% of the data points have a quality rating of 7 or less, so the model was simply picking 7 or less for everything and managing to get 98% accurate. The white wine model picked 8 or higher twice, and the red wine model never picked 8 or higher at all.\n",
    "\n",
    "Finally, to have a model that actually made decisions, we lowered the bar for binary classification by having the model predict if the quality score would be higher than a 6. This lowered the accuracy from the previous model (**90.0%** for red wine, **78.5%** for white wine), but at least the model was actually trying to predict when the quality rating was 7 or greater."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965cb03c",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Final accuracy on testing data: **90.0%** for red wine, **78.5%** for white wine\n",
    "\n",
    "Given the features of red wine, we are able to accurately predict if the quality rating of the wine is going greater than a 6. \n",
    "\n",
    "Given the features of white wine, we are able to *relatively* accurately predict if the quality rating of the wine is going greater than a 6. \n",
    "\n",
    "This can be used to identify common characteristics of quality wine, and to give a sense of if a new wine will be good before tasting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a9606bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imblearn\n",
    "# !pip install xgboost\n",
    "\n",
    "# FOR MAC USERS\n",
    "# !brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f48a7895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED WINE RANDOM FOREST 0.9047619047619048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95       352\n",
      "           1       0.60      0.57      0.59        47\n",
      "\n",
      "    accuracy                           0.90       399\n",
      "   macro avg       0.77      0.76      0.77       399\n",
      "weighted avg       0.90      0.90      0.90       399\n",
      "\n",
      "WHITE WINE RANDOM FOREST 0.874897792313982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92       952\n",
      "           1       0.69      0.78      0.73       271\n",
      "\n",
      "    accuracy                           0.87      1223\n",
      "   macro avg       0.81      0.84      0.83      1223\n",
      "weighted avg       0.88      0.87      0.88      1223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Step 1: Apply SMOTE to balance the training datasets\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# For red wine\n",
    "X_red_train_balanced, y_red_train_balanced = smote.fit_resample(X_red_train, y_red_train_binary)\n",
    "\n",
    "# For white wine\n",
    "X_white_train_balanced, y_white_train_balanced = smote.fit_resample(X_white_train, y_white_train_binary)\n",
    "\n",
    "# Step 2: Train ensemble classifiers\n",
    "# Random Forest for red wine\n",
    "rf_model_red = RandomForestClassifier(random_state=42)\n",
    "rf_model_red.fit(X_red_train_balanced, y_red_train_balanced)\n",
    "\n",
    "# Random Forest for white wine\n",
    "rf_model_white = RandomForestClassifier(random_state=42)\n",
    "rf_model_white.fit(X_white_train_balanced, y_white_train_balanced)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_red_pred_rf = rf_model_red.predict(X_red_test)\n",
    "y_white_pred_rf = rf_model_white.predict(X_white_test)\n",
    "\n",
    "# Calculate accuracy and classification reports for the Random Forest models\n",
    "red_rf_accuracy = accuracy_score(y_red_test_binary, y_red_pred_rf)\n",
    "white_rf_accuracy = accuracy_score(y_white_test_binary, y_white_pred_rf)\n",
    "red_rf_report = classification_report(y_red_test_binary, y_red_pred_rf)\n",
    "white_rf_report = classification_report(y_white_test_binary, y_white_pred_rf)\n",
    "\n",
    "print(\"RED WINE RANDOM FOREST\", red_rf_accuracy)\n",
    "print(red_rf_report)\n",
    "print(\"WHITE WINE RANDOM FOREST\", white_rf_accuracy)\n",
    "print(white_rf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "343be10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for red wine: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best F1 score for red wine: 0.930635316964905\n",
      "Best parameters for white wine: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Best F1 score for white wine: 0.9135122769989888\n",
      "RED WINE BEST RANDOM FOREST 0.9022556390977443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94       352\n",
      "           1       0.58      0.60      0.59        47\n",
      "\n",
      "    accuracy                           0.90       399\n",
      "   macro avg       0.76      0.77      0.77       399\n",
      "weighted avg       0.90      0.90      0.90       399\n",
      "\n",
      "WHITE WINE BEST RANDOM FOREST 0.8773507767784138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       952\n",
      "           1       0.71      0.76      0.73       271\n",
      "\n",
      "    accuracy                           0.88      1223\n",
      "   macro avg       0.82      0.84      0.83      1223\n",
      "weighted avg       0.88      0.88      0.88      1223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for the red wine Random Forest model\n",
    "grid_search_red = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='f1',  # Focus on F1 score for balanced evaluation\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model for red wine\n",
    "grid_search_red.fit(X_red_train_balanced, y_red_train_balanced)\n",
    "\n",
    "# Set up GridSearchCV for the white wine Random Forest model\n",
    "grid_search_white = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model for white wine\n",
    "grid_search_white.fit(X_white_train_balanced, y_white_train_balanced)\n",
    "\n",
    "# Best parameters and results\n",
    "print(\"Best parameters for red wine:\", grid_search_red.best_params_)\n",
    "print(\"Best F1 score for red wine:\", grid_search_red.best_score_)\n",
    "\n",
    "print(\"Best parameters for white wine:\", grid_search_white.best_params_)\n",
    "print(\"Best F1 score for white wine:\", grid_search_white.best_score_)\n",
    "\n",
    "# Evaluate the best models on the test set\n",
    "best_rf_red = grid_search_red.best_estimator_\n",
    "best_rf_white = grid_search_white.best_estimator_\n",
    "\n",
    "y_red_pred_best_rf = best_rf_red.predict(X_red_test)\n",
    "y_white_pred_best_rf = best_rf_white.predict(X_white_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"RED WINE BEST RANDOM FOREST\", accuracy_score(y_red_test_binary, y_red_pred_best_rf))\n",
    "print(classification_report(y_red_test_binary, y_red_pred_best_rf))\n",
    "\n",
    "print(\"WHITE WINE BEST RANDOM FOREST\", accuracy_score(y_white_test_binary, y_white_pred_best_rf))\n",
    "print(classification_report(y_white_test_binary, y_white_pred_best_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a765dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shane\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\core.py:158: UserWarning: [13:49:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Shane\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\core.py:158: UserWarning: [13:49:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for red wine: {'colsample_bytree': 0.6, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 300, 'subsample': 1.0}\n",
      "Best F1 score for red wine: 0.9447635924983652\n",
      "Best parameters for white wine: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300, 'subsample': 0.7}\n",
      "Best F1 score for white wine: 0.8962524486723691\n",
      "RED WINE BEST XGBOOST MODEL ACCURACY: 0.9022556390977443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94       352\n",
      "           1       0.59      0.55      0.57        47\n",
      "\n",
      "    accuracy                           0.90       399\n",
      "   macro avg       0.77      0.75      0.76       399\n",
      "weighted avg       0.90      0.90      0.90       399\n",
      "\n",
      "WHITE WINE BEST XGBOOST MODEL ACCURACY: 0.8798037612428454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92       952\n",
      "           1       0.72      0.74      0.73       271\n",
      "\n",
      "    accuracy                           0.88      1223\n",
      "   macro avg       0.82      0.83      0.83      1223\n",
      "weighted avg       0.88      0.88      0.88      1223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'subsample': [0.7, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Step 1: Set up and run GridSearchCV for red wine XGBoost model\n",
    "grid_search_xgb_red = GridSearchCV(\n",
    "    estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    param_grid=param_grid_xgb,\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model for red wine\n",
    "grid_search_xgb_red.fit(X_red_train_balanced, y_red_train_balanced)\n",
    "\n",
    "# Step 2: Set up and run GridSearchCV for white wine XGBoost model\n",
    "grid_search_xgb_white = GridSearchCV(\n",
    "    estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    param_grid=param_grid_xgb,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model for white wine\n",
    "grid_search_xgb_white.fit(X_white_train_balanced, y_white_train_balanced)\n",
    "\n",
    "# Step 3: Get the best parameters and results\n",
    "print(\"Best parameters for red wine:\", grid_search_xgb_red.best_params_)\n",
    "print(\"Best F1 score for red wine:\", grid_search_xgb_red.best_score_)\n",
    "\n",
    "print(\"Best parameters for white wine:\", grid_search_xgb_white.best_params_)\n",
    "print(\"Best F1 score for white wine:\", grid_search_xgb_white.best_score_)\n",
    "\n",
    "# Step 4: Evaluate the best models on the test set\n",
    "best_xgb_red = grid_search_xgb_red.best_estimator_\n",
    "best_xgb_white = grid_search_xgb_white.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_red_pred_best_xgb = best_xgb_red.predict(X_red_test)\n",
    "y_white_pred_best_xgb = best_xgb_white.predict(X_white_test)\n",
    "\n",
    "# Calculate and print accuracy and classification reports\n",
    "print(\"RED WINE BEST XGBOOST MODEL ACCURACY:\", accuracy_score(y_red_test_binary, y_red_pred_best_xgb))\n",
    "print(classification_report(y_red_test_binary, y_red_pred_best_xgb))\n",
    "\n",
    "print(\"WHITE WINE BEST XGBOOST MODEL ACCURACY:\", accuracy_score(y_white_test_binary, y_white_pred_best_xgb))\n",
    "print(classification_report(y_white_test_binary, y_white_pred_best_xgb))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
