{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fce42611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red train D:   (1200, 11)\n",
      "Red train y:   (1200,)\n",
      "Red test D:    (399, 11)\n",
      "Red test y:    (399,)\n",
      "White train D: (3675, 11)\n",
      "White train y: (3675,)\n",
      "White test D:  (1223, 11)\n",
      "White test y:  (1223,)\n",
      "Column headers: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "\n",
    "\n",
    "# Load the data sets\n",
    "D_red   = np.loadtxt(open(\"winequality-red.csv\"), delimiter = \";\", skiprows = 1)\n",
    "D_white = np.loadtxt(open(\"winequality-white.csv\"), delimiter = \";\", skiprows = 1)\n",
    "\n",
    "\n",
    "red_cols = open(\"winequality-red.csv\", \"r\").readline().replace(\"\\n\", \"\").replace('\"', '').split(\";\")\n",
    "white_cols = open(\"winequality-white.csv\", \"r\").readline().replace(\"\\n\", \"\").replace('\"', '').split(\";\")\n",
    "\n",
    "\n",
    "# Shuffle the datasets\n",
    "np.random.shuffle(D_red)\n",
    "np.random.shuffle(D_white)\n",
    "\n",
    "\n",
    "# 75% train, 25% test\n",
    "D_red_train   = D_red[:1200]\n",
    "D_red_test    = D_red[1200:]\n",
    "\n",
    "D_white_train = D_white[:3675]\n",
    "D_white_test  = D_white[3675:]\n",
    "\n",
    "\n",
    "# Separate features and actual quality\n",
    "y_red_train = D_red_train[:, 11]\n",
    "D_red_train = np.delete(D_red_train, 11, 1)\n",
    "y_red_test  = D_red_test[:, 11]\n",
    "D_red_test  = np.delete(D_red_test, 11, 1)\n",
    "\n",
    "y_white_train = D_white_train[:, 11]\n",
    "D_white_train = np.delete(D_white_train, 11, 1)\n",
    "y_white_test  = D_white_test[:, 11]\n",
    "D_white_test  = np.delete(D_white_test, 11, 1)\n",
    "\n",
    "# Check shapes of data frames\n",
    "print(\"Red train D:  \", D_red_train.shape)\n",
    "print(\"Red train y:  \", y_red_train.shape)\n",
    "print(\"Red test D:   \", D_red_test.shape)\n",
    "print(\"Red test y:   \", y_red_test.shape)\n",
    "\n",
    "print(\"White train D:\", D_white_train.shape)\n",
    "print(\"White train y:\", y_white_train.shape)\n",
    "print(\"White test D: \", D_white_test.shape)\n",
    "print(\"White test y: \", y_white_test.shape)\n",
    "\n",
    "print(\"Column headers:\", red_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "918d3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the features for both datasets (helps improve model performance)\n",
    "scaler = StandardScaler()\n",
    "X_red_train = scaler.fit_transform(D_red_train)\n",
    "X_red_test = scaler.transform(D_red_test)\n",
    "X_white_train = scaler.fit_transform(D_white_train)\n",
    "X_white_test = scaler.transform(D_white_test)\n",
    "\n",
    "# Convert quality ratings to binary classification (0 = 7 or below, 1 = above 7)\n",
    "# For red wine\n",
    "y_red_train_binary = (y_red_train > 6).astype(int)\n",
    "y_red_test_binary = (y_red_test > 6).astype(int)\n",
    "\n",
    "# For white wine\n",
    "y_white_train_binary = (y_white_train > 6).astype(int)\n",
    "y_white_test_binary = (y_white_test > 6).astype(int)\n",
    "\n",
    "# Create new logistic regression models for binary classification\n",
    "model_red_binary = LogisticRegression(max_iter=500)\n",
    "model_white_binary = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the binary models\n",
    "model_red_binary.fit(X_red_train, y_red_train_binary)\n",
    "model_white_binary.fit(X_white_train, y_white_train_binary)\n",
    "\n",
    "# Make predictions on the test set for binary classification\n",
    "y_red_pred_binary = model_red_binary.predict(X_red_test)\n",
    "y_white_pred_binary = model_white_binary.predict(X_white_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3d75f707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED WINE BINARY CLASSIFICATION\n",
      "    Accuracy: 0.87468671679198\n",
      "    Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93       342\n",
      "           1       0.60      0.37      0.46        57\n",
      "\n",
      "    accuracy                           0.87       399\n",
      "   macro avg       0.75      0.66      0.69       399\n",
      "weighted avg       0.86      0.87      0.86       399\n",
      "\n",
      "    Confusion matrix:\n",
      "[[328  14]\n",
      " [ 36  21]]\n",
      "    Attribute weights:\n",
      "        0.822 \t- alcohol\n",
      "        0.708 \t- sulphates\n",
      "        0.621 \t- fixed acidity\n",
      "        0.311 \t- residual sugar\n",
      "        0.153 \t- free sulfur dioxide\n",
      "        0.117 \t- pH\n",
      "        -0.089 \t- citric acid\n",
      "        -0.24 \t- chlorides\n",
      "        -0.514 \t- density\n",
      "        -0.575 \t- volatile acidity\n",
      "        -0.76 \t- total sulfur dioxide\n",
      "\n",
      "WHITE WINE BINARY CLASSIFICATION\n",
      "    Accuracy: 0.8152085036794767\n",
      "    Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.89       959\n",
      "           1       0.68      0.27      0.39       264\n",
      "\n",
      "    accuracy                           0.82      1223\n",
      "   macro avg       0.76      0.62      0.64      1223\n",
      "weighted avg       0.80      0.82      0.78      1223\n",
      "\n",
      "    Confusion matrix:\n",
      "[[926  33]\n",
      " [193  71]]\n",
      "    Attribute weights:\n",
      "        1.293 \t- residual sugar\n",
      "        0.48 \t- pH\n",
      "        0.414 \t- fixed acidity\n",
      "        0.251 \t- alcohol\n",
      "        0.246 \t- sulphates\n",
      "        0.164 \t- free sulfur dioxide\n",
      "        -0.026 \t- total sulfur dioxide\n",
      "        -0.121 \t- citric acid\n",
      "        -0.235 \t- chlorides\n",
      "        -0.339 \t- volatile acidity\n",
      "        -1.656 \t- density\n",
      "\n",
      "AVG ATTRIBUTE WEIGHTS\n",
      "        0.802 \t- residual sugar\n",
      "        0.536 \t- alcohol\n",
      "        0.518 \t- fixed acidity\n",
      "        0.477 \t- sulphates\n",
      "        0.298 \t- pH\n",
      "        0.158 \t- free sulfur dioxide\n",
      "        -0.105 \t- citric acid\n",
      "        -0.238 \t- chlorides\n",
      "        -0.393 \t- total sulfur dioxide\n",
      "        -0.457 \t- volatile acidity\n",
      "        -1.085 \t- density\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display accuracy scores\n",
    "red_accuracy_binary = accuracy_score(y_red_test_binary, y_red_pred_binary)\n",
    "white_accuracy_binary = accuracy_score(y_white_test_binary, y_white_pred_binary)\n",
    "\n",
    "# Calculate and display classification reports\n",
    "red_report_binary = classification_report(y_red_test_binary, y_red_pred_binary)\n",
    "white_report_binary = classification_report(y_white_test_binary, y_white_pred_binary)\n",
    "\n",
    "# Get regression coefficients\n",
    "red_attr_weights = {}\n",
    "white_attr_weights = {}\n",
    "avg_attr_weights = {}\n",
    "for i in range(len(red_cols) - 1):\n",
    "    red_attr_weights[red_cols[i]] = round(model_red_binary.coef_[0][i], 3)\n",
    "    white_attr_weights[white_cols[i]] = round(model_white_binary.coef_[0][i], 3)    \n",
    "    avg_attr_weights[red_cols[i]] = (red_attr_weights[red_cols[i]] + white_attr_weights[red_cols[i]]) / 2\n",
    "\n",
    "# Output regression coefficients\n",
    "print(\"RED WINE BINARY CLASSIFICATION\")\n",
    "print(\"    Accuracy:\", red_accuracy_binary)\n",
    "print(\"    Report:\\n\" + str(red_report_binary))\n",
    "print(\"    Confusion matrix:\\n\" + str(confusion_matrix(y_red_test_binary, y_red_pred_binary)))\n",
    "print(\"    Attribute weights:\")\n",
    "for i in sorted(red_attr_weights.items(), key=lambda x: -(x[1])):\n",
    "    print(\"        \" + str(i[1]), \"\\t-\", i[0])\n",
    "\n",
    "print()\n",
    "print(\"WHITE WINE BINARY CLASSIFICATION\")\n",
    "print(\"    Accuracy:\", white_accuracy_binary)\n",
    "print(\"    Report:\\n\" + str(white_report_binary))\n",
    "print(\"    Confusion matrix:\\n\" + str(confusion_matrix(y_white_test_binary, y_white_pred_binary)))\n",
    "print(\"    Attribute weights:\")\n",
    "for i in sorted(white_attr_weights.items(), key=lambda x: -(x[1])):\n",
    "    print(\"        \" + str(i[1]), \"\\t-\", i[0])\n",
    "\n",
    "print()\n",
    "print(\"AVG ATTRIBUTE WEIGHTS\")\n",
    "for i in sorted(avg_attr_weights.items(), key=lambda x: -x[1]):\n",
    "    print(\"        \" + str(round(i[1], 3)), \"\\t-\", i[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d3926a5",
   "metadata": {},
   "source": [
    "# Initial Questions\n",
    "### 1. Which attributes affect the quality rating more than others?\n",
    "\n",
    "To find the attributes that impact the quality rating the most, we can look at the regression coefficients. Since there is no bias for this model, the equation behind the scenes to guess the quality score is:\n",
    "\n",
    "&omega;<sub>1</sub>x<sub>1</sub> + &omega;<sub>2</sub>x<sub>2</sub> + ... + &omega;<sub>j</sub>x<sub>j</sub> = y<sub>i</sub>\n",
    "\n",
    "where &omega; is the matrix of regression coefficients, x is the matrix of attribute values, and y<sub>i</sub> is the quality prediction. The larger the magnitude of a regression coefficient is, the larger its impact on the quality rating is. The more positive a regression coefficient is, it has more of a positive impact on the quality of the wine. The more negative it is, it has more of a negative impact on the quality. \n",
    "\n",
    "Since there are two different models (red and white wine), there are two different sets of regression coefficients. We can then average the coefficients for each attribute to get the overall impact rating of an attribute, and rank them in sorted order. After averaging the results over 20 trials, this leads us with the following:\n",
    "\n",
    "Attributes that make a positive impact, from most to least impactful: \n",
    "1. residual sugar: 0.8174\n",
    "2. alcohol: 0.55085\n",
    "3. sulphates: 0.4703\n",
    "4. fixed acidity: 0.404425\n",
    "5. pH: 0.25375\n",
    "6. free sulfur dioxide: 0.139875\n",
    "7. citric acid: 0.008725\n",
    "\n",
    "Attributes that make a negative impact, from most to least impactful: \n",
    "1. density: -1.076925\n",
    "2. volatile acidity: -0.424075\n",
    "3. chlorides: -0.342825\n",
    "4. total sulfur dioxide: -0.30405\n",
    "\n",
    "### 2. Given its attributes, is it possible to predict if a wine will be \"high\" quality (>7)?\n",
    "\n",
    "Originally, we wanted to see if we could predict the exact rating of a wine, but this quickly proved to be tough. Our model wasn't yielding high accuracy (~40%). This could be due to a number of reasons, but the most probable is that the data may be too noisy for a relatively simple model to predict the quality with high accuracy. \n",
    "\n",
    "We then tried putting the ratings in \"buckets\" and have the model try to predict the score within a range. The buckets were quality scores of 5 and below, 6 or 7, and 8 and above. This is due to the fact that the largest concentration of quality scores in the dataset are in the 6-8 range. This approach yielded slightly higher accuracy than the first idea, getting closer to 60%.\n",
    "\n",
    "Next, we decided to try to predict if a quality score will be higher than a 7. This would simplify the work the model has to do by giving it a binary classification problem. Using this approach, our model was able to reach 98.7% accuracy with the red wine dataset (1,200 training points, 399 testing points), and 97.9% accuracy with the white wine dataset (3,675 training points, 1,223 testing points). The only problem, was that the model wasn't actually doing any guessing. Due to the nature of the dataset, it just so happens that 98% of the data points have a quality rating of 7 or less, so the model was simply picking 7 or less for everything and managing to get 98% accurate. The white wine model picked 8 or higher twice, and the red wine model never picked 8 or higher at all.\n",
    "\n",
    "Finally, to have a model that actually made decisions, we lowered the bar for binary classification by having the model predict if the quality score would be higher than a 6. This lowered the accuracy from the previous model (**87.81%** for red wine, **80.18%** for white wine)**, but at least the model was actually trying to predict when the quality rating was 7 or greater.\n",
    "\n",
    "**NOTE: averaged over 20 trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965cb03c",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Final accuracy on testing data: **87.81%** for red wine, **80.18%** for white wine\n",
    "\n",
    "Given the features of red wine, we are able to accurately predict if the quality rating of the wine is going greater than a 6. \n",
    "\n",
    "Given the features of white wine, we are able to *relatively* accurately predict if the quality rating of the wine is going greater than a 6. \n",
    "\n",
    "This can be used to identify common characteristics of quality wine, and to give a sense of if a new wine will be good before tasting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a9606bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imblearn\n",
    "# !pip install xgboost\n",
    "\n",
    "# FOR MAC USERS\n",
    "# !brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f48a7895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED WINE RANDOM FOREST 0.9047619047619048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95       352\n",
      "           1       0.60      0.57      0.59        47\n",
      "\n",
      "    accuracy                           0.90       399\n",
      "   macro avg       0.77      0.76      0.77       399\n",
      "weighted avg       0.90      0.90      0.90       399\n",
      "\n",
      "WHITE WINE RANDOM FOREST 0.874897792313982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92       952\n",
      "           1       0.69      0.78      0.73       271\n",
      "\n",
      "    accuracy                           0.87      1223\n",
      "   macro avg       0.81      0.84      0.83      1223\n",
      "weighted avg       0.88      0.87      0.88      1223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Step 1: Apply SMOTE to balance the training datasets\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# For red wine\n",
    "X_red_train_balanced, y_red_train_balanced = smote.fit_resample(X_red_train, y_red_train_binary)\n",
    "\n",
    "# For white wine\n",
    "X_white_train_balanced, y_white_train_balanced = smote.fit_resample(X_white_train, y_white_train_binary)\n",
    "\n",
    "# Step 2: Train ensemble classifiers\n",
    "# Random Forest for red wine\n",
    "rf_model_red = RandomForestClassifier(random_state=42)\n",
    "rf_model_red.fit(X_red_train_balanced, y_red_train_balanced)\n",
    "\n",
    "# Random Forest for white wine\n",
    "rf_model_white = RandomForestClassifier(random_state=42)\n",
    "rf_model_white.fit(X_white_train_balanced, y_white_train_balanced)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_red_pred_rf = rf_model_red.predict(X_red_test)\n",
    "y_white_pred_rf = rf_model_white.predict(X_white_test)\n",
    "\n",
    "# Calculate accuracy and classification reports for the Random Forest models\n",
    "red_rf_accuracy = accuracy_score(y_red_test_binary, y_red_pred_rf)\n",
    "white_rf_accuracy = accuracy_score(y_white_test_binary, y_white_pred_rf)\n",
    "red_rf_report = classification_report(y_red_test_binary, y_red_pred_rf)\n",
    "white_rf_report = classification_report(y_white_test_binary, y_white_pred_rf)\n",
    "\n",
    "print(\"RED WINE RANDOM FOREST\", red_rf_accuracy)\n",
    "print(red_rf_report)\n",
    "print(\"WHITE WINE RANDOM FOREST\", white_rf_accuracy)\n",
    "print(white_rf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "343be10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for red wine: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best F1 score for red wine: 0.930635316964905\n",
      "Best parameters for white wine: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Best F1 score for white wine: 0.9135122769989888\n",
      "RED WINE BEST RANDOM FOREST 0.9022556390977443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94       352\n",
      "           1       0.58      0.60      0.59        47\n",
      "\n",
      "    accuracy                           0.90       399\n",
      "   macro avg       0.76      0.77      0.77       399\n",
      "weighted avg       0.90      0.90      0.90       399\n",
      "\n",
      "WHITE WINE BEST RANDOM FOREST 0.8773507767784138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       952\n",
      "           1       0.71      0.76      0.73       271\n",
      "\n",
      "    accuracy                           0.88      1223\n",
      "   macro avg       0.82      0.84      0.83      1223\n",
      "weighted avg       0.88      0.88      0.88      1223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for the red wine Random Forest model\n",
    "grid_search_red = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='f1',  # Focus on F1 score for balanced evaluation\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model for red wine\n",
    "grid_search_red.fit(X_red_train_balanced, y_red_train_balanced)\n",
    "\n",
    "# Set up GridSearchCV for the white wine Random Forest model\n",
    "grid_search_white = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model for white wine\n",
    "grid_search_white.fit(X_white_train_balanced, y_white_train_balanced)\n",
    "\n",
    "# Best parameters and results\n",
    "print(\"Best parameters for red wine:\", grid_search_red.best_params_)\n",
    "print(\"Best F1 score for red wine:\", grid_search_red.best_score_)\n",
    "\n",
    "print(\"Best parameters for white wine:\", grid_search_white.best_params_)\n",
    "print(\"Best F1 score for white wine:\", grid_search_white.best_score_)\n",
    "\n",
    "# Evaluate the best models on the test set\n",
    "best_rf_red = grid_search_red.best_estimator_\n",
    "best_rf_white = grid_search_white.best_estimator_\n",
    "\n",
    "y_red_pred_best_rf = best_rf_red.predict(X_red_test)\n",
    "y_white_pred_best_rf = best_rf_white.predict(X_white_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"RED WINE BEST RANDOM FOREST\", accuracy_score(y_red_test_binary, y_red_pred_best_rf))\n",
    "print(classification_report(y_red_test_binary, y_red_pred_best_rf))\n",
    "\n",
    "print(\"WHITE WINE BEST RANDOM FOREST\", accuracy_score(y_white_test_binary, y_white_pred_best_rf))\n",
    "print(classification_report(y_white_test_binary, y_white_pred_best_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a765dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shane\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\core.py:158: UserWarning: [13:49:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Shane\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\core.py:158: UserWarning: [13:49:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for red wine: {'colsample_bytree': 0.6, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 300, 'subsample': 1.0}\n",
      "Best F1 score for red wine: 0.9447635924983652\n",
      "Best parameters for white wine: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300, 'subsample': 0.7}\n",
      "Best F1 score for white wine: 0.8962524486723691\n",
      "RED WINE BEST XGBOOST MODEL ACCURACY: 0.9022556390977443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94       352\n",
      "           1       0.59      0.55      0.57        47\n",
      "\n",
      "    accuracy                           0.90       399\n",
      "   macro avg       0.77      0.75      0.76       399\n",
      "weighted avg       0.90      0.90      0.90       399\n",
      "\n",
      "WHITE WINE BEST XGBOOST MODEL ACCURACY: 0.8798037612428454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92       952\n",
      "           1       0.72      0.74      0.73       271\n",
      "\n",
      "    accuracy                           0.88      1223\n",
      "   macro avg       0.82      0.83      0.83      1223\n",
      "weighted avg       0.88      0.88      0.88      1223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'subsample': [0.7, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Step 1: Set up and run GridSearchCV for red wine XGBoost model\n",
    "grid_search_xgb_red = GridSearchCV(\n",
    "    estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    param_grid=param_grid_xgb,\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model for red wine\n",
    "grid_search_xgb_red.fit(X_red_train_balanced, y_red_train_balanced)\n",
    "\n",
    "# Step 2: Set up and run GridSearchCV for white wine XGBoost model\n",
    "grid_search_xgb_white = GridSearchCV(\n",
    "    estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    param_grid=param_grid_xgb,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model for white wine\n",
    "grid_search_xgb_white.fit(X_white_train_balanced, y_white_train_balanced)\n",
    "\n",
    "# Step 3: Get the best parameters and results\n",
    "print(\"Best parameters for red wine:\", grid_search_xgb_red.best_params_)\n",
    "print(\"Best F1 score for red wine:\", grid_search_xgb_red.best_score_)\n",
    "\n",
    "print(\"Best parameters for white wine:\", grid_search_xgb_white.best_params_)\n",
    "print(\"Best F1 score for white wine:\", grid_search_xgb_white.best_score_)\n",
    "\n",
    "# Step 4: Evaluate the best models on the test set\n",
    "best_xgb_red = grid_search_xgb_red.best_estimator_\n",
    "best_xgb_white = grid_search_xgb_white.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_red_pred_best_xgb = best_xgb_red.predict(X_red_test)\n",
    "y_white_pred_best_xgb = best_xgb_white.predict(X_white_test)\n",
    "\n",
    "# Calculate and print accuracy and classification reports\n",
    "print(\"RED WINE BEST XGBOOST MODEL ACCURACY:\", accuracy_score(y_red_test_binary, y_red_pred_best_xgb))\n",
    "print(classification_report(y_red_test_binary, y_red_pred_best_xgb))\n",
    "\n",
    "print(\"WHITE WINE BEST XGBOOST MODEL ACCURACY:\", accuracy_score(y_white_test_binary, y_white_pred_best_xgb))\n",
    "print(classification_report(y_white_test_binary, y_white_pred_best_xgb))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
